{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f46lxLSsBp0f"
   },
   "source": [
    "# Семинар: Градиентный спуск. Задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "z8Nfs9riBp0k"
   },
   "outputs": [],
   "source": [
    "from typing import Iterable, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5OOYVQOBp0l"
   },
   "source": [
    "## Часть 1. Градиентный спуск (вспомним формулы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7j2vGNTyBp0l"
   },
   "source": [
    "Функционал ошибки, который мы применяем в задаче регрессии — Mean Squared Error:\n",
    "\n",
    "$$\n",
    "Q(w, X, y) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^\\ell (\\langle x_i, w \\rangle - y_i)^2\n",
    "$$\n",
    "\n",
    "где $x_i$ — это $i$-ый объект датасета, $y_i$ — правильный ответ для $i$-го объекта, а $w$ — веса нашей линейной модели.\n",
    "\n",
    "Можно показать, что для линейной модели, функционал ошибки можно записать в матричном виде следующим образом:\n",
    "$$\n",
    "Q(w, X, y) =\\frac{1}{l} (y - Xw)^T(y-Xw)\n",
    "$$\n",
    "или\n",
    "$$\n",
    "Q(w, X, y) = \\frac{1}{l} || Xw - y ||^2\n",
    "$$\n",
    "\n",
    "где $X$ — это матрица объекты-признаки, а $y$ — вектор правильных ответов\n",
    "\n",
    "Для того чтобы воспользоваться методом градиентного спуска, нам нужно посчитать градиент нашего функционала. Для MSE он будет выглядеть так:\n",
    "\n",
    "$$\n",
    "\\nabla_w Q(w, X, y) = \\frac{2}{l} X^T(Xw-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формула для одной итерации градиентного спуска выглядит следующим образом:\n",
    "\n",
    "$$\n",
    "w^t = w^{t-1} - \\eta \\nabla_{w} Q(w^{t-1}, X, y)\n",
    "$$\n",
    "\n",
    "Где $w^t$ — значение вектора весов на $t$-ой итерации, а $\\eta$ — параметр learning rate, отвечающий за размер шага."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7jS7bJUBp0s"
   },
   "source": [
    "## Часть 2. Линейная регрессия (продолжение части 1 - используются решения из части 1). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRrNZdJ7Bp0s"
   },
   "source": [
    "Создадим класс для линейной регрессии. Он будет использовать интерфейс, знакомый нам из библиотеки `sklearn`.\n",
    "\n",
    "В методе `fit` мы будем подбирать веса `w` при помощи градиентного спуска нашим методом `gradient_descent`.\n",
    "\n",
    "В методе `predict` мы будем применять нашу регрессию к датасету, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyKvDkPOBp0s"
   },
   "source": [
    "**Задание 2.1:** Допишите код в методах `fit` и `predict` класса `LinearRegression_1`\n",
    "\n",
    "В методе `fit` вам нужно инициализировать веса `w` (например, из члучайного распределения), применить наш `gradient_descent` и сохранить последние веса `w` из траектории.\n",
    "\n",
    "В методе `predict` вам нужно применить линейную регрессию и вернуть вектор ответов.\n",
    "\n",
    "Обратите внимание, что объект лосса (функционала ошибки) передаётся в момент инициализации и хранится в `self.loss`. Его нужно использовать в `fit` для `gradient_descent` (например, с `n_iterations: int = 1000`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "\n",
    "class BaseLoss(abc.ABC):\n",
    "    \"\"\"Базовый класс лосса\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Функция для вычислений значения лосса\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :return: число -- значения функции потерь\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для вычислений градиента лосса по весам w\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :return: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KdTk79QTBp0t"
   },
   "outputs": [],
   "source": [
    "class LinearRegression_1:\n",
    "    def __init__(self, loss: BaseLoss, lr: float = 0.01) -> None: \n",
    "        #loss - функционал ошибки\n",
    "        #lr - градиентный шаг\n",
    "        self.loss = loss\n",
    "        self.lr = lr\n",
    "        self.w = None  # Веса модели\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, n_iterations: int = 1000) -> \"LinearRegression_1\":\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        # Добавляем столбец из единиц для константного признака\n",
    "        X = np.hstack([X, np.ones([X.shape[0], 1])])\n",
    "\n",
    "        # -- YOUR CODE HERE --  \n",
    "        # Инициализация весов из равномерного распределения\n",
    "        self.w = np.random.rand(X.shape[1])\n",
    "\n",
    "        for _ in range(n_iterations):\n",
    "            # Вычисление градиента\n",
    "            gradient = (2 / X.shape[0]) * X.T @ (X @ self.w - y)\n",
    "            # Обновление веса по формуле градиентного спуска\n",
    "            self.w = self.w - self.lr * gradient\n",
    "        return self   \n",
    "\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        # Проверяем, что регрессия обучена, то есть, что был вызван fit и в нём был установлен атрибут self.w\n",
    "        assert hasattr(self, \"w\"), \"Linear regression must be fitted first\"\n",
    "        # Добавляем столбец из единиц для константного признака\n",
    "        X = np.hstack([X, np.ones([X.shape[0], 1])])\n",
    "\n",
    "        # -- YOUR CODE HERE --\n",
    "        \n",
    "        # Предсказывание значения\n",
    "        predictions = X @ self.w\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CV2RGHwgBp0t"
   },
   "source": [
    "Класс линейной регрессии создан. Более того, мы можем управлять тем, какой функционал ошибки мы оптимизируем, просто передавая разные классы в параметр `loss` при инициализации. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIG_P49YBp0t"
   },
   "source": [
    "Будем применять нашу регрессию на реальном датасете. Загрузим датасет с машинами (см. семинар 5_sem-sklearn-knn.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nJn0SUlnBp0t"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_raw = pd.read_csv(\n",
    "    \"http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\",\n",
    "    header=None,      #в исх. таблице нет названий колонок\n",
    "    na_values=[\"?\"],  #если ?, то NaN\n",
    ")\n",
    "\n",
    "X_raw.head()\n",
    "X_raw = X_raw[~X_raw[25].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_KcER5JtBp0u"
   },
   "outputs": [],
   "source": [
    "y = X_raw[25]\n",
    "X_raw = X_raw.drop(25, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVS4T-IUBp0u"
   },
   "source": [
    "**Задание 2.2:** Обработайте датасет нужными методами, чтобы на нём можно было обучать линейную регрессию (см. семинар 5_sem-sklearn-knn.ipynb):\n",
    "\n",
    "* Заполните пропуски средними (библиотека SimpleImputer)\n",
    "* Переведите категориальные признаки в числовые (в методе get_dummies использовать drop_first=True.)\n",
    "* Разделите датасет на обучающую и тестовую выборку (задать: доля тестовой выборки равна 0.3, `random_state=42`, `shuffle=True`)\n",
    "* Нормализуйте числовые признаки (при помощи бибилиотеки StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "COp1ybClBp0u"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Разделение на числовые и категориальные признаки\n",
    "numeric_features = X_raw.select_dtypes(include=[np.number]).columns\n",
    "categorical_features = X_raw.select_dtypes(include=[object]).columns\n",
    "\n",
    "# Создание препроцессора для числовых и категориальных признаков\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(drop=\"first\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Объединение препроцессоров\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Разделение на обучающую и тестовую выборку\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_raw, y, test_size=0.3, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Применение препроцессора к обучающей выборке\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Применение препроцессора к тестовой выборке\n",
    "X_test_processed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qm7c7zIzBp0u"
   },
   "source": [
    "**Задание 2.3:** Обучите написанную вами линейную регрессию на обучающей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуем класс MSELoss\n",
    "class MSELoss(BaseLoss):\n",
    "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Функция для вычислений значения лосса\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :return: число -- значения функции потерь\n",
    "        \"\"\"\n",
    "        # -- YOUR CODE HERE --\n",
    "        # Вычислите значение функции потерь при помощи X, y и w и верните его\n",
    "        l = X.shape[0]\n",
    "        return np.dot((y - X@w).T, (y - X@w)) / l\n",
    "  \n",
    "\n",
    "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для вычислений градиента лосса по весам w\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :return: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
    "        \"\"\"\n",
    "        # -- YOUR CODE HERE --\n",
    "        # Вычислите значение вектора градиента при помощи X, y и w и верните его\n",
    "        l = X.shape[0]\n",
    "        return 2 * X.T @ (X @ w - y) / l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем объект линейной регрессии для `MSELoss`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Cyfkw-L1Bp0t"
   },
   "outputs": [],
   "source": [
    "lr_1 = LinearRegression_1(MSELoss()) #создаем регрессор\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "lzjAlpliBp0u"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --\n",
    "\n",
    "# Обучаем регрессор на обучающей выборке\n",
    "lr_1.fit(X_train_processed, y_train)\n",
    "\n",
    "# Предсказываем значения на обучающей выборке\n",
    "y_train_pred = lr_1.predict(X_train_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rr1bF2_bBp0u"
   },
   "source": [
    "**Задание 2.4:** Посчитайте ошибку обученной регрессии на обучающей и тестовой выборке при помощи методов `mean_squared_error` и `r2_score` из `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на обучающей выборке: 4027842.3495557\n",
      "R^2 на обучающей выборке: 0.916401216290347\n",
      "MSE на тестовой выборке: 11062903.552126575\n",
      "R^2 на тестовой выборке: 0.8832924356609586\n"
     ]
    }
   ],
   "source": [
    "# -- YOUR CODE HERE --\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Создаем объект линейной регрессии\n",
    "lr_1 = LinearRegression_1(MSELoss())\n",
    "\n",
    "# Обучаем регрессор на обучающей выборке\n",
    "lr_1.fit(X_train_processed, y_train)\n",
    "\n",
    "# Предсказываем значения на обучающей выборке\n",
    "y_train_pred = lr_1.predict(X_train_processed)\n",
    "\n",
    "# Предсказываем значения на тестовой выборке\n",
    "y_test_pred = lr_1.predict(X_test_processed)\n",
    "\n",
    "# Вычисляем MSE на обучающей выборке\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "print(f\"MSE на обучающей выборке: {mse_train}\")\n",
    "\n",
    "# Вычисляем R^2 на обучающей выборке\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "print(f\"R^2 на обучающей выборке: {r2_train}\")\n",
    "\n",
    "# Вычисляем MSE на тестовой выборке\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"MSE на тестовой выборке: {mse_test}\")\n",
    "\n",
    "# Вычисляем R^2 на тестовой выборке\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "print(f\"R^2 на тестовой выборке: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SS5Oh4YoBp0v"
   },
   "source": [
    "Добавим к модели L2 регуляризацию (для борьбы с переобучением). Для этого нам нужно создать новый класс для нового функционала ошибки и его градиента.\n",
    "\n",
    "Формула функционала ошибки для MSE с L2 регуляризацией выглядит так:\n",
    "$$\n",
    "Q(w, X, y) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^\\ell (\\langle x_i, w \\rangle - y_i)^2 + \\lambda ||w||^2\n",
    "$$\n",
    "\n",
    "Или в матричном виде:\n",
    "\n",
    "$$\n",
    "Q(w, X, y) = \\frac{1}{\\ell} || Xw - y ||^2 + \\lambda ||w||^2,\n",
    "$$\n",
    "\n",
    "где $\\lambda$ — коэффициент регуляризации\n",
    "\n",
    "Заметим, что (удобно для вычислений):\n",
    "$$\n",
    "Q(w, X, y) = \\frac{1}{\\ell} || Xw - y ||^2+\\lambda ||w||^2 =\\frac{1}{l} (y - Xw)^T(y-Xw)+ \\lambda w^Tw.\n",
    "$$\n",
    "\n",
    "Градиент $\\nabla_w Q(w, X, y)$ выглядит так:\n",
    "\n",
    "$$\n",
    "\\nabla_w Q(w, X, y) = \\frac{2}{\\ell} X^T(Xw-y) + 2 \\lambda w\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vogzj2W0Bp0v"
   },
   "source": [
    "**Задание 2.5:** Реализуйте класс `MSEL2Loss`\n",
    "\n",
    "Он должен вычислять значение функционала ошибки (лосс) $\n",
    "Q(w, X, y)$ и его градиент $\\nabla_w Q(w, X, y)$ по формулам (выше).\n",
    "\n",
    "Подсказка: обратите внимание, что последний элемент вектора `w` — это bias (сдвиг) (в классе `LinearRegression` к матрице `X` добавляется колонка из единиц — константный признак). bias регуляризовать не нужно. Поэтому не забудьте убрать последний элемент из `w` при подсчёте слагаемого $\\lambda||w||^2$ в `calc_loss` и занулить его при подсчёте слагаемого $2 \\lambda w$ в `calc_grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "58FvnEjHBp0v"
   },
   "outputs": [],
   "source": [
    "class MSEL2Loss(BaseLoss):\n",
    "    def __init__(self, coef: float = 1.0):\n",
    "        \"\"\"\n",
    "        :param coef: коэффициент регуляризации (лямбда в формуле)\n",
    "        \"\"\"\n",
    "        self.coef = coef\n",
    "\n",
    "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Функция для вычислений значения лосса\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета. Последний признак константный.\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии. Последний вес -- bias.\n",
    "        :output: число -- значения функции потерь\n",
    "        \"\"\"    \n",
    "        # -- YOUR CODE HERE --\n",
    "        # Вычислите значение функции потерь при помощи X, y и w и верните его\n",
    "        \n",
    "        predictions = np.dot(X, w)\n",
    "        residuals = predictions - y\n",
    "        loss = np.mean(residuals**2) + self.coef * np.sum(w[:-1]**2)  # Учитываем регуляризацию, не затрагивая bias\n",
    "        return loss\n",
    "\n",
    "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для вычислений градиента лосса по весам w\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :output: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
    "        \"\"\"\n",
    "        # -- YOUR CODE HERE --\n",
    "        # Вычислите значение вектора градиента при помощи X, y и w и верните его\n",
    "        \n",
    "        predictions = np.dot(X, w)\n",
    "        residuals = predictions - y\n",
    "        gradient = (2 / len(y)) * np.dot(X.T, residuals) + 2 * self.coef * np.hstack([w[:-1], [0]])  # Зануляем градиент для bias\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27410487.5\n",
      "[1163180. 1172283. 1181386. 1190489. 1199592. 1208695. 1217798. 1226901.\n",
      " 1236004. 1245089.]\n",
      "Всё верно!\n"
     ]
    }
   ],
   "source": [
    "#Проверка:\n",
    "#Создадим объект лосса\n",
    "losst = MSEL2Loss()\n",
    "\n",
    "# Создадим датасет\n",
    "Xt = np.arange(200).reshape(20, 10)\n",
    "yt = np.arange(20)\n",
    "\n",
    "# Создадим вектор весов\n",
    "wt = np.arange(10)\n",
    "\n",
    "#print(Xt)\n",
    "#print(yt)\n",
    "#print(wt)\n",
    "\n",
    "# Выведем значение лосса и градиента на этом датасете с этим вектором весов\n",
    "print(losst.calc_loss(Xt, yt, wt))\n",
    "print(losst.calc_grad(Xt, yt, wt))\n",
    "\n",
    "# Проверка, что методы реализованы правильно\n",
    "assert losst.calc_loss(Xt, yt, wt) == 27410487.5, \"Метод calc_loss реализован неверно\" \n",
    "assert np.allclose(\n",
    "    losst.calc_grad(Xt, yt, wt),\n",
    "    np.array(\n",
    "        [\n",
    "            1163180.0,\n",
    "            1172283.0,\n",
    "            1181386.0,\n",
    "            1190489.0,\n",
    "            1199592.0,\n",
    "            1208695.0,\n",
    "            1217798.0,\n",
    "            1226901.0,\n",
    "            1236004.0,\n",
    "            1245089.0,\n",
    "        ]\n",
    "    ),\n",
    "), \"Метод calc_grad реализован неверно\"\n",
    "\n",
    "print(\"Всё верно!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTl1XQP5Bp0v"
   },
   "source": [
    "Теперь мы можем использовать лосс с l2 регуляризацией в нашей регрессии. Пусть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "QanspiddBp0v"
   },
   "outputs": [],
   "source": [
    "lr_1_l2 = LinearRegression_1(MSEL2Loss(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwmC8B0OBp0w"
   },
   "source": [
    "**Задание 2.6:** Обучите регрессию с лоссом `MSEL2Loss`. Попробуйте использовать другие коэффициенты регуляризации. Получилось ли улучшить разультат на тестовой выборке? Сравните результат применения регрессии с регуляризацией к обучающей и тестовой выборкам с результатом применения регрессии без регуляризации к тем же выборкам.(Для оценки качества использовать методы `mean_squared_error` и `r2_score` из `sklearn.metrics`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "9FId8tKOBp0w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на тестовой выборке с L2-регуляризацией: 11063024.216822745\n",
      "R^2 на тестовой выборке с L2-регуляризацией: 0.8832911627146001\n"
     ]
    }
   ],
   "source": [
    "# -- YOUR CODE HERE --\n",
    "\n",
    "# Создаем объект линейной регрессии с L2-регуляризацией, коэффициент регуляризации 1000\n",
    "lr_1_l2 = LinearRegression_1(MSEL2Loss(coef=1000))\n",
    "\n",
    "# Обучаем регрессор на обучающей выборке\n",
    "lr_1_l2.fit(X_train_processed, y_train)\n",
    "\n",
    "# Предсказываем значения на тестовой выборке\n",
    "y_pred_l2 = lr_1_l2.predict(X_test_processed)\n",
    "\n",
    "# Сравниваем результаты с регрессией без регуляризации\n",
    "mse_test_l2 = mean_squared_error(y_test, y_pred_l2)\n",
    "r2_test_l2 = r2_score(y_test, y_pred_l2)\n",
    "\n",
    "print(f\"MSE на тестовой выборке с L2-регуляризацией: {mse_test_l2}\")\n",
    "print(f\"R^2 на тестовой выборке с L2-регуляризацией: {r2_test_l2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на тестовой выборке с L2-регуляризацией: 11062595.026644185\n",
      "R^2 на тестовой выборке с L2-регуляризацией: 0.8832956904355679\n"
     ]
    }
   ],
   "source": [
    "# Коэффициент регуляризации 0.001\n",
    "lr_1_l2 = LinearRegression_1(MSEL2Loss(coef=0.001))\n",
    "\n",
    "lr_1_l2.fit(X_train_processed, y_train)\n",
    "\n",
    "y_pred_l2 = lr_1_l2.predict(X_test_processed)\n",
    "\n",
    "mse_test_l2 = mean_squared_error(y_test, y_pred_l2)\n",
    "r2_test_l2 = r2_score(y_test, y_pred_l2)\n",
    "\n",
    "print(f\"MSE на тестовой выборке с L2-регуляризацией: {mse_test_l2}\")\n",
    "print(f\"R^2 на тестовой выборке с L2-регуляризацией: {r2_test_l2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заменить, значение коэффициента регуляризации практически не влияет на модель."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "l5cGx9vyBp0x"
   ],
   "name": "hw05-gd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
